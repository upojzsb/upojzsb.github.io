<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>upojzsb's blog</title>
    <description>Justice delaied is justice denied</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 07 Oct 2020 22:59:58 +0800</pubDate>
    <lastBuildDate>Wed, 07 Oct 2020 22:59:58 +0800</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>统计学习方法读书笔记 Ch7 支持向量机（Support Vector Machine）</title>
        <description>&lt;h1 id=&quot;svm-模型&quot;&gt;SVM 模型&lt;/h1&gt;

&lt;p&gt;SVM 是一种二分类模型。假设训练集中的样本是线性可分的， SVM 的目的是找到一个分类超平面，使得间隔最大化。满足间隔最大化的解是唯一的。&lt;/p&gt;

&lt;p&gt;根据模型的复杂程度，SVM可分为线性可分支持向量机、线性支持向量机和非线性支持向量机。&lt;/p&gt;

&lt;h1 id=&quot;函数间隔和几何间隔&quot;&gt;函数间隔和几何间隔&lt;/h1&gt;

&lt;p&gt;对数据进行预测，该点离分类超平面越远说明本次预测的置信度越高，反之说明置信度低。在分类超平面确定时，可以通过 $ | \omega \cdot x + b | $ 反映点 $ x $ 和分类超平面的距离，由于符号表明本次预测的分类结果，所以可以用 $ y ( \omega \cdot x + b ) $ 同时对正确性和置信度进行描述。&lt;/p&gt;

&lt;p&gt;称：&lt;/p&gt;

\[\hat{\gamma}_i = y_i ( \omega \cdot x_i + b )\]

&lt;p&gt;为超平面关于样本点的函数间隔，同时定义超平面关于数据集的函数间隔 $ \hat{\gamma} $ 为所有样本点的函数间隔的最小值。&lt;/p&gt;

&lt;p&gt;但使用函数间隔来选择超平面还不够完善，只需要给 $ \omega $ 和 $ b $ 同时乘一个系数，就可以使分类超平面不变的情况下，使函数间隔就任意大。&lt;/p&gt;

&lt;p&gt;可以添加一些约束条件使得间隔确定。称：&lt;/p&gt;

\[\gamma_i = y_i ( \frac{\omega}{||\omega||} \cdot x_i + \frac{b}{||\omega||} )\]

&lt;p&gt;为超平面关于样本点的几何间隔，同时定义超平面关于数据集的几何间隔 $ \gamma $ 为所有样本点的几何间隔的最小值。&lt;/p&gt;

&lt;p&gt;从以上两式可以得知函数间隔和几何间隔之间的关系：&lt;/p&gt;

\[\gamma = \frac{\hat{\gamma}}{||\omega||}\]

&lt;p&gt;当 $ ||\omega|| = 1 $ 时，函数间隔和几何间隔一致。&lt;/p&gt;

&lt;h1 id=&quot;线性可分-svm-模型&quot;&gt;线性可分 SVM 模型&lt;/h1&gt;

&lt;h2 id=&quot;模型&quot;&gt;模型&lt;/h2&gt;

&lt;p&gt;给定线性可分的训练集，通过间隔最大化找出来的分类超平面为：&lt;/p&gt;

\[\omega^{*}\cdot x +b^{*} = 0\]

&lt;p&gt;分类决策函数为：&lt;/p&gt;

\[f(x) = sign(\omega^{*}\cdot x +b^{*})\]

&lt;p&gt;该模型称为线性可分支持向量机。&lt;/p&gt;

&lt;h2 id=&quot;间隔最大化的方法&quot;&gt;间隔最大化的方法&lt;/h2&gt;

&lt;p&gt;首先描述以下这个问题：&lt;/p&gt;

\[\begin{align}

\max_{\omega, b}\ &amp;amp; \gamma \\

s.t.\ &amp;amp; y_i ( \frac{\omega}{||\omega||} \cdot x_i + \frac{b}{||\omega||} )\ge \gamma , i = 1, 2, ..., N

\end{align}\]

&lt;p&gt;根据函数间隔和几何间隔之间的关系，改写为：&lt;/p&gt;

\[\begin{align}

\max_{\omega, b}\ &amp;amp; \frac{\hat{\gamma}}{||\omega||} \\

s.t.\ &amp;amp; y_i ( \omega \cdot x_i + b )\ge \hat{\gamma} , i = 1, 2, ..., N

\end{align}\]

&lt;p&gt;由于函数间隔可以在不改变超平面的情况下任意缩放，所以令 $ \hat{\gamma} = 1 $，得到：&lt;/p&gt;

\[\begin{align}

\max_{\omega, b}\ &amp;amp; \frac{1}{||\omega||} \\

s.t.\ &amp;amp; y_i ( \omega \cdot x_i + b ) - 1 \ge 0 , i = 1, 2, ..., N

\end{align}\]

&lt;p&gt;考虑到凸函数更好进行优化，最大化 $ \frac{1}{||\omega||} $ 和最小化 $ \frac{1}{2}||\omega||^2 $ 问题可以看作等价的，得到：&lt;/p&gt;

\[\begin{align}

\min_{\omega, b}\ &amp;amp; \frac{1}{2}||\omega||^2 \\

s.t.\ &amp;amp; y_i ( \omega \cdot x_i + b )\ge \hat{\gamma} , i = 1, 2, ..., N

\end{align}\]

&lt;p&gt;构成了一个凸二次规划问题。&lt;/p&gt;

&lt;p&gt;构建 Lagrange 函数，并对其对偶问题进行求解就可以得到本问题中线性可分支持向量机的参数。&lt;/p&gt;

&lt;h2 id=&quot;支持向量&quot;&gt;支持向量&lt;/h2&gt;

&lt;p&gt;在线性可分的情况下，距离超平面最近的样本称为支持向量，根据上一节谈到的约束条件，在超平面：&lt;/p&gt;

\[\omega \cdot x + b = 1\]

&lt;p&gt;的正例和在超平面&lt;/p&gt;

\[\omega \cdot x + b = -1\]

&lt;p&gt;的负例构成支持向量，这两个超平面的距离为 $  \frac{2}{||\omega||} $。&lt;/p&gt;

&lt;p&gt;在决定分离超平面时，只有支持向量起作用，其它实例点不起作用。&lt;/p&gt;

&lt;h1 id=&quot;-&quot;&gt;$ $&lt;/h1&gt;
&lt;h1 id=&quot;线性-svm&quot;&gt;线性 SVM&lt;/h1&gt;

&lt;h2 id=&quot;模型-1&quot;&gt;模型&lt;/h2&gt;

&lt;p&gt;由于实际数据中存在误差和噪声，同时考虑到不同数据服从的分布不同，所以实际数据几乎没有线性可分的。在线性不可分的情况下上文中所讲的：&lt;/p&gt;

\[\begin{align}

\min_{\omega, b}\ &amp;amp; \frac{1}{2}||\omega||^2 \\

s.t.\ &amp;amp; y_i ( \omega \cdot x_i + b )\ge 1 , i = 1, 2, ..., N

\end{align}\]

&lt;p&gt;中的约束项就不满足，优化问题就进行不下去了。&lt;/p&gt;

&lt;p&gt;考虑到这个问题，我们可以对数据集引入松弛变量 $ \xi_i \ge 0 $ 使约束条件变为：&lt;/p&gt;

\[y_i ( \omega \cdot x_i + b )\ge 1 - \xi_i , i = 1, 2, ..., N\]

&lt;p&gt;然后，令目标问题为：&lt;/p&gt;

\[\frac{1}{2}||\omega||^2 + C\sum_{i=1}^{N}\xi_i\]

&lt;p&gt;式中， $ C \gt 0 $ 为惩罚参数。这样对误分类的要求降低，同时可以通过控制惩罚参数来经可能降低误分类点。&lt;/p&gt;

&lt;p&gt;整个优化问题可以写为：&lt;/p&gt;

\[\begin{align}

\min_{\omega, b}\ &amp;amp; \frac{1}{2}||\omega||^2 + C\sum_{i=1}^{N}\xi_i \\

s.t.\ &amp;amp; y_i ( \omega \cdot x_i + b )\ge 1 - \xi_i , i = 1, 2, ..., N \\
&amp;amp; \xi_i \ge 0, i = 1, 2, \dots, N

\end{align}\]

&lt;p&gt;这样就可以针对线性不可分的数据集进行训练了。&lt;/p&gt;

&lt;h1 id=&quot;--1&quot;&gt;$ $&lt;/h1&gt;
&lt;h1 id=&quot;一些对我来说不显然的地方&quot;&gt;一些对我来说不显然的地方&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;高中的立体几何忘得差不多了 :-(&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;1--omega-cdot-x--b--0--为什么是平面&quot;&gt;1. $ \omega \cdot x + b = 0 $ 为什么是平面&lt;/h2&gt;

&lt;p&gt;已知平面 $ P $ ，设其法向量为 $ \omega $，设 $ Q $ 为平面内一定点，$ A $ 为平面内任意一点，有：&lt;/p&gt;

\[\begin{align}
\vec{AQ}\perp\vec{\omega} \Rightarrow&amp;amp; \vec{AQ} \cdot \vec{\omega} = 0 \\
                          \Rightarrow&amp;amp; (\vec{OQ} - \vec{OA}) \cdot \vec{\omega} = 0 \\
                          \Rightarrow&amp;amp; \vec{OQ} \cdot \vec{\omega} - \vec{OA} \cdot \vec{\omega} = 0
\end{align}\]

&lt;p&gt;令 $ x = \vec{OQ}, b = -\vec{OA} \cdot \vec{\omega}$ 则有：&lt;/p&gt;

\[\omega \cdot x + b = 0\]

&lt;p&gt;显然，对于所有的 $ x \in P $ ，均有 $ \omega \cdot x + b = 0 $ ；若 $ x \not\in P $ ，则必然 $ \omega \cdot x + b \neq 0 $ 。&lt;/p&gt;

&lt;p&gt;故 $ \omega \cdot x + b = 0 $ 表示一个平面。&lt;/p&gt;

&lt;h2 id=&quot;2---omega-cdot-x--b---为什么反映了平面外一点到平面的距离&quot;&gt;2. $ | \omega \cdot x + b | $ 为什么反映了平面外一点到平面的距离&lt;/h2&gt;

&lt;p&gt;已知平面 $ P $ ，设其法向量为 $ \omega $，设 $ Q $ 为平面内 $ \omega $ 的起点，$ A $ 为平面外一点，如下图所示，
&lt;img src=&quot;/img/post/slm/sim.ch7-distance_dot2plane.jpg&quot; alt=&quot;Distance Between Dot and Plane&quot; /&gt;&lt;/p&gt;

&lt;p&gt;显然，$ A $ 点在 $ \omega $ 方向上的投影 $ A’ $ 到 $ Q $ 点之间的距离 $ d $ 为 $ A $ 点到平面 $ P $ 的距离。&lt;/p&gt;

&lt;p&gt;设 $ \vec{QA} $ 和 $ \omega $ 的夹角为 $ \theta $ ，则有：&lt;/p&gt;

\[\begin{align}
d =&amp;amp; |\vec{QA}| \cos \theta \\
  =&amp;amp; |\vec{QA}| \frac{ \vec{QA} \cdot \vec{\omega} }{|\vec{QA}|\times|\vec{\omega}|} \\
  =&amp;amp; \frac{ \vec{QA} \cdot \vec{\omega} }{|\vec{\omega}|} \\
  =&amp;amp; \frac{1}{|\vec{\omega}|}(\vec{\omega}\cdot\vec{OA}-\vec{\omega}\cdot\vec{OQ}) \\

由于Q \in P，所以\vec{OQ} \cdot \vec{\omega} + b = 0，故：\\

  =&amp;amp; \frac{1}{|\vec{\omega}|}(\vec{\omega}\cdot\vec{OA}+b) \\

\end{align}\]

&lt;p&gt;故$ | \omega \cdot x + b | $ 为什么反映了平面外一点到平面的距离。&lt;/p&gt;

&lt;h1 id=&quot;--2&quot;&gt;$ $&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;Initial push @ 2020-09-28&lt;/p&gt;

  &lt;p&gt;Update 1 @ 2020-10-07&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 28 Sep 2020 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2020/09/28/Statistical-learning-method-Ch7-Support-Vector-Machine/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/09/28/Statistical-learning-method-Ch7-Support-Vector-Machine/</guid>
        
        <category>Statistical learning method</category>
        
        <category>Study notes</category>
        
        <category>Support Vector Machine</category>
        
        <category>SVM</category>
        
        
      </item>
    
      <item>
        <title>《信息论基础》中文第二版勘误</title>
        <description>&lt;h1 id=&quot;书籍信息&quot;&gt;书籍信息&lt;/h1&gt;

&lt;p&gt;书名：信息论基础（原书第二版）&lt;/p&gt;

&lt;p&gt;原名：Elements of Information Theory, Second Edition&lt;/p&gt;

&lt;p&gt;出版社：机械工业出版社&lt;/p&gt;

&lt;p&gt;版次：2018年6月第一版第11次印刷&lt;/p&gt;

&lt;p&gt;ISBN：978-7-111-22040-4&lt;/p&gt;

&lt;h1 id=&quot;第二章&quot;&gt;第二章&lt;/h1&gt;

&lt;h2 id=&quot;p14&quot;&gt;P14&lt;/h2&gt;

&lt;p&gt;（2-72）下方&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;…如果仅当 $ \lambda = 0 $ 或 $ \lambda = 1 $，上式成立…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;原文：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;A function f is said to be strictly convex if equality holds only if λ = 0
or λ = 1.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;应改为：&lt;/p&gt;

&lt;p&gt;…如果仅当 $ \lambda = 0 $ 或 $ \lambda = 1 $，等号成立…&lt;/p&gt;

&lt;h2 id=&quot;p15&quot;&gt;P15&lt;/h2&gt;

&lt;p&gt;（2-89）下方&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;…由于 $ \log t $ 是关于 $ t $ 的严格凸函数…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;原文&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Since $ log t $ is a strictly concave function of $ t $,&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;应改为&lt;/p&gt;

&lt;p&gt;…由于 $ \log t $ 是关于 $ t $ 的严格凹函数…&lt;/p&gt;

&lt;h1 id=&quot;第五章&quot;&gt;第五章&lt;/h1&gt;

&lt;h2 id=&quot;p60&quot;&gt;P60&lt;/h2&gt;

&lt;p&gt;例5.1.3下方的定义&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;…则称这个编码是非奇异的（nonsigular）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;原文&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;A code is said to be nonsingular…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;应改为&lt;/p&gt;

&lt;p&gt;…则称这个编码是非奇异的（nonsingular）。&lt;/p&gt;
</description>
        <pubDate>Thu, 17 Sep 2020 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2020/09/17/Mistake-Correction-of-Elements-of-Information-Theory-Chinese-Version-Second-Edition/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/09/17/Mistake-Correction-of-Elements-of-Information-Theory-Chinese-Version-Second-Edition/</guid>
        
        <category>Mistake Correction</category>
        
        <category>Elements of Information Theory</category>
        
        
      </item>
    
      <item>
        <title>统计学习方法读书笔记 Ch6 逻辑斯蒂回归与最大熵模型（Logistic Regression and Maximum Entropy Model）</title>
        <description>&lt;h1 id=&quot;logistic-模型&quot;&gt;Logistic 模型&lt;/h1&gt;

&lt;p&gt;Logistic 回归是一种分类模型，它针对于服从 Logistic 分布的随机变量进行建模。&lt;/p&gt;

&lt;h2 id=&quot;logistic-分布&quot;&gt;Logistic 分布&lt;/h2&gt;

&lt;p&gt;Logistic 分布的分布函数和概率密度函数如下：&lt;/p&gt;

\[P(X \le x) = \frac{1}{1+e^{-(x-\mu)/\gamma}} \\

p(x) = \frac{e^{-(x-\mu)/\gamma}}{\gamma (1+e^{-(x-\mu)/\gamma})^2}\]

&lt;p&gt;式中，$ \mu $ 为位置参数， $ \gamma \gt 0 $ 为形状参数。&lt;/p&gt;

&lt;p&gt;Logistic 分布的概率密度函数为钟形曲线，分布函数为 S 形曲线 ( sigmoid )。&lt;/p&gt;

&lt;h2 id=&quot;二项-logistic-回归&quot;&gt;二项 Logistic 回归&lt;/h2&gt;

&lt;h3 id=&quot;模型&quot;&gt;模型&lt;/h3&gt;

&lt;p&gt;二项 Logistic 回归可以对 $ P(Y|X) $ 建模，其中 $ X $ 为输入特征， $ Y $ 为分类结果，具体模型为：&lt;/p&gt;

\[\begin{align}

P(Y=1|x) =&amp;amp; \frac{\exp(\omega \cdot x + b)}{1+\exp(\omega \cdot x + b)} \\

P(Y=0|x) =&amp;amp; \frac{1}{1+\exp(\omega \cdot x + b)}

\end{align}\]

&lt;p&gt;其中， 输入特征 $ x \in R^n $，输出 $ y \in \lbrace 0, 1 \rbrace $，参数包括 $ \omega \in R^n $ 和 $ b \in R $。&lt;/p&gt;

&lt;p&gt;为了方便起见，重新定义 $ \omega = (\omega^{(1)}, …, \omega^{(n)}, b)^T $ 和 $ x = (x^{(1)}, …, x^{(n)}, 1)^T $ ，此时 $ \omega \cdot x + b $ 可以改写为 $ \omega \cdot x $ 。&lt;/p&gt;

&lt;p&gt;考虑 Logistic 回归模型的输入和输出，不难发现，它将实数域内的的值 $ \omega \cdot x $ 转换为概率 $ P(Y=1|x) = \frac{\exp(\omega \cdot x)}{1+\exp(\omega \cdot x)}  $，即输入越接近 $ +\infty $ 概率越接近1，输入越接近 $ -\infty $ 概率越接近0。&lt;/p&gt;

&lt;h3 id=&quot;参数估计&quot;&gt;参数估计&lt;/h3&gt;

&lt;p&gt;针对训练集&lt;/p&gt;

\[T = \lbrace (x_1, y_1), ..., (x_N, y_N) \rbrace\]

&lt;p&gt;可以利用极大似然估计法对模型的参数进行估计。&lt;/p&gt;

&lt;p&gt;构建对数似然函数：&lt;/p&gt;

\[\begin{align}

L(\omega) =&amp;amp; \log \prod_{i=1}^{N} (P(Y=1|x))^{y_i} (P(Y=0|x))^{1-y_i} \\

=&amp;amp; \sum_{i=1}^N \big( y_i \log P(Y=1|x) + (1-y_i)P(Y=0|x) \big) \\

=&amp;amp; \sum_{i=1}^{N} \big(  y_i(\omega \cdot x_i) - \log (1|\exp (\omega \cdot x_i)) \big)

\end{align}\]

&lt;p&gt;最大化 $ L(\omega) $ 就可以得到 $ \omega $ 的估计值。&lt;/p&gt;

&lt;h2 id=&quot;多项-logistic-回归&quot;&gt;多项 Logistic 回归&lt;/h2&gt;

&lt;p&gt;假设随机变量 $ Y $ 的取值范围为 \lbrace 1, …, K \rbrace，则多项 Logistic 回归模型为：&lt;/p&gt;

\[\begin{align}

P(Y=k|x) =&amp;amp; \frac{\exp(\omega_k\cdot x)}{1+\sum_{i=1}^{K-1}\exp (\omega_i\cdot x)}, k=1, ..., K-1 \\

P(Y=K|x) =&amp;amp; \frac{1}{1+\sum_{i=1}^{K-1}\exp (\omega_i\cdot x)}

\end{align}\]

&lt;h1 id=&quot;最大熵模型&quot;&gt;最大熵模型&lt;/h1&gt;

&lt;p&gt;最大熵模型认为，在所有可能的概率模型中，熵最大的模型是最好的模型。&lt;/p&gt;
</description>
        <pubDate>Wed, 16 Sep 2020 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2020/09/16/Statistical-learning-method-Ch6-Logistic-Regression-and-Maximum-Entropy-Model/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/09/16/Statistical-learning-method-Ch6-Logistic-Regression-and-Maximum-Entropy-Model/</guid>
        
        <category>Statistical learning method</category>
        
        <category>Study notes</category>
        
        <category>Logistic Regression</category>
        
        <category>Maximum Entropy Model</category>
        
        
      </item>
    
      <item>
        <title>统计学习方法读书笔记 Ch5 决策树（Decision tree）</title>
        <description>&lt;h1 id=&quot;模型&quot;&gt;模型&lt;/h1&gt;

&lt;p&gt;决策树是一种基础的分类和回归方法，这里仅对其分类作用进行讨论。决策树模型呈现树状结构，基于特征对实例进行分类，它可以认为是 if-then 规则的集合。下图是利用 Sklearn 实现的决策树对 MNIST 数据集进行分类的可视化结果，可以很容易的从中看出 if-then 结构。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/post/slm/slm-ch5-dt-mnist.png&quot; alt=&quot;MNIST&quot; /&gt;&lt;/p&gt;

&lt;p&gt;决策树由节点和有向边组成，节点分为内部节点（表示一个特征或属性）和外部节点（表示一个类）。&lt;/p&gt;

&lt;p&gt;给定训练集&lt;/p&gt;

\[D = \lbrace (x_1, y_1), ..., (x_N, y_N)\rbrace\]

&lt;p&gt;其中 $ x_i $ 为输入的特征向量， $ y_i $ 为类标签， $ N $ 为样本容量。决策树学习的目标就是根据训练集构建一个决策树模型，使其可以对实例进行正确的分类。&lt;/p&gt;

&lt;h1 id=&quot;特征选择&quot;&gt;特征选择&lt;/h1&gt;

&lt;p&gt;特征选择的目的是从训练集中选择具有分类能力的特征，用来划分特征空间。&lt;/p&gt;

&lt;h2 id=&quot;熵与信息增益&quot;&gt;熵与信息增益&lt;/h2&gt;

&lt;p&gt;熵是随机变量不确定性的度量，定义为：&lt;/p&gt;

\[H(X) = -\sum_{i=1}^{n} p_i \log p_i\]

&lt;p&gt;其中 $ p_i = P(X=x_i), i = 1, …, n $，熵越大，随机变量的不确定性越大。&lt;/p&gt;

&lt;p&gt;定义条件熵为：&lt;/p&gt;

\[H(Y|X) = \sum_{i=1}^{n} p_i H(Y|X=x_i)\]

&lt;p&gt;则可以定义信息增益 $ g $ 为：&lt;/p&gt;

\[g(D, A) = H(D) - H(D|A)\]

&lt;p&gt;信息增益反映了在了解特征 $ A $ 的信息后，使得类 $ Y $ 信息的不确定性减少的程度。在进行特征选择时，选择信息增益较大的特征作为分类条件会使其具有更强的分类能力。&lt;/p&gt;

&lt;p&gt;由于信息增益会倾向于选择取值较多的特征，所以可以采用信息增益比来对此问题进行矫正。&lt;/p&gt;

&lt;p&gt;信息增益比定义为：&lt;/p&gt;

\[g_R(D, A) = \frac{g(D, A)}{H_A(D)} = \frac{g(D, A)}{-\sum_{i=1}^{n}\frac{|D_i|}{D} \log \frac{|D_i|}{D}}\]

&lt;h1 id=&quot;剪枝&quot;&gt;剪枝&lt;/h1&gt;

&lt;p&gt;决策树的生成算法会递归产生决策树，直到不能继续下去。这种算法倾向与构建复杂的决策树，使训练集的分类很准确，但会降低模型的泛化能力。对于这一问题的解决方法是考虑决策树的复杂度，对已生成的决策树进行简化。&lt;/p&gt;

&lt;p&gt;这种简化的过程称为剪枝，其具体过程是通过某种方法，从已经生成的决策树上裁掉一些子树或叶子结点，使其父节点作为新的叶子结点。&lt;/p&gt;

&lt;p&gt;可以定义决策树的损失函数，并通过最小化损失函数来实现剪枝。&lt;/p&gt;

&lt;p&gt;首先定义经验熵：&lt;/p&gt;

\[H_t = -\sum_k \frac{N_{tk}}{N_t}\log \frac{N_{tk}}{N_t}\]

&lt;p&gt;定义模型对训练数据的预测误差：&lt;/p&gt;

\[C(T) = \sum_{t=1}^{|T|}N_tH_t(T) = -\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk}\log \frac{N_{tk}}{N_t}\]

&lt;p&gt;最后，定义损失函数为：&lt;/p&gt;

\[C_{\alpha}(T) = C(T) +\alpha |T|\]

&lt;p&gt;其中，树 $ T $ 的叶子节点数量为 $ |T| $ ， $ t $ 是 $ T $ 的叶子节点，该节点由 $ N_t $ 个样本点，其中 $ k $ 类样本点由 $ N_{tk} $ 个， $ \alpha \le 0 $ 为参数。&lt;/p&gt;
</description>
        <pubDate>Mon, 14 Sep 2020 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2020/09/14/Statistical-learning-method-Ch5-Decision-tree/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/09/14/Statistical-learning-method-Ch5-Decision-tree/</guid>
        
        <category>Statistical learning method</category>
        
        <category>Study notes</category>
        
        <category>Decision tree</category>
        
        
      </item>
    
      <item>
        <title>统计学习方法读书笔记 Ch4 朴素贝叶斯法（Naive Bayes）</title>
        <description>&lt;h1 id=&quot;模型&quot;&gt;模型&lt;/h1&gt;

&lt;p&gt;朴素贝叶斯算法是一种基于贝叶斯定理和特征条件独立假设的分类算法。其接受待分类实例的特征向量作为输入，对该实例分类的结果作为输出。具体定义如下：&lt;/p&gt;

&lt;p&gt;存在随机变量 $ X, Y $ 构成联合概率分布 $ P(X, Y) $ 可以生成训练集&lt;/p&gt;

\[T = \lbrace (x_1, y_1), ..., (x_N, y_N) \rbrace\]

&lt;p&gt;其中 $ x \in R^n , y_i \in \lbrace c_1, …, c_K \rbrace $，$ K $ 为分类的数量。&lt;/p&gt;

&lt;p&gt;朴素贝叶斯就是对联合概率分布 $ P(X,Y) $ 的学习。具体的，可以根据先验概率&lt;/p&gt;

\[P(Y=c_k)\]

&lt;p&gt;及条件概率&lt;/p&gt;

\[P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)}, ..., X^{(n)}=x^{(n)}|Y=c_k)\]

&lt;p&gt;学习联合分布概率 $ P(X, Y) $。&lt;/p&gt;

&lt;p&gt;如果对所有 $ x \in  R^n $ 建模，则该模型的参数量会随着 $ x $ 的复杂程度指数增长，所以，一般会对模型进行简化，即进行独立条件假设：&lt;/p&gt;

&lt;p&gt;将&lt;/p&gt;

\[\begin{align}
P(X=x|Y=c_k) =&amp;amp; P(X^{(1)}=x^{(1)}, ..., X^{(n)}=x^{(n)}|Y=c_k) \\
=&amp;amp; P(X^{(1)}=x^{(1)}|X^{(2)}=x^{(2)}, ..., X^{(n)}=x^{(n)},Y=c_k)\cdot \\
&amp;amp; ... \\
&amp;amp; P(X^{(n-1)}=x^{(n-1)}|X^{(n)}=x^{(n)},Y=c_k) \cdot \\
&amp;amp; P(X^{(n)}=x^{(n)}|Y=c_k)
\end{align}\]

&lt;p&gt;简化为&lt;/p&gt;

\[\begin{align}
P(X=x|Y=c_k) =&amp;amp; P(X^{(1)}=x^{(1)}, ..., X^{(n)}=x^{(n)}|Y=c_k) \\
=&amp;amp; \prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)
\end{align}\]

&lt;p&gt;独立性假设认为 $ X^{(i)} $ 和 $ X^{(j)} (i \ne j)$ 之间无关，这会简化朴素贝叶斯，但同样会牺牲一定的准确率（例如在垃圾邮件分类中，词语“安全账户”和“警告”同时出现的概率与高于两词分别出现）。&lt;/p&gt;

&lt;p&gt;在学习了联合概率分布后，当输入一个新实例时 $ x \in R^n $ ，会对其后验概率进行计算：&lt;/p&gt;

\[\begin{align}
P(Y=c_k|X=x) =&amp;amp; \frac{P(X=x|Y=c_k) \cdot P(Y=c_k)}{P(X=x)} \\
=&amp;amp; \frac{P(X=x|Y=c_k) \cdot P(Y=c_k)}
        {\sum_{k=1}^{K}P(X=x|Y=c_k)P(Y=c_k)} \\
=&amp;amp; \frac{\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k) \cdot P(Y=c_k)}
        {\sum_{k=1}^{K}\Bigg(\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)P(Y=c_k)\Bigg)} \\
\end{align}\]

&lt;p&gt;由于对于任意实例，&lt;/p&gt;

\[\sum_{k=1}^{K}\Bigg(\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)P(Y=c_k)\Bigg)\]

&lt;p&gt;为常数，所以朴素贝叶斯分类器表示为：&lt;/p&gt;

\[y = f(x) = \arg \max_{c_k} \prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k) \cdot P(Y=c_k)\]

&lt;h1 id=&quot;参数估计&quot;&gt;参数估计&lt;/h1&gt;

&lt;p&gt;先验概率 $ P(Y=c_k) $ 实质上是样本集 $ T $ 中，分类为 $ c_k $ 的样本的占比，即：&lt;/p&gt;

\[P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i = c_k)}{N}\]

&lt;p&gt;设 $ a \in S $ 为实例特征向量的取值范围，条件概率 $ P(X^{(i)}=a|Y=c_k) $ 实质上是样本集 $ T $ 中，分类为 $c_k $ 且 $ X^{(i)} $ 取 $ a $ 的样本数量占分类为 $ c_k $的比例，即：&lt;/p&gt;

\[P(X^{(i)}=a\|Y=c_k) = \frac{\sum_{j=1}^{N}I(X^{(i)} = a , y_j = c_k)}{\sum_{i=1}^{N}I(y_i = c_k )}\]

&lt;h1 id=&quot;拉普拉斯平滑&quot;&gt;拉普拉斯平滑&lt;/h1&gt;

&lt;p&gt;利用贝叶斯对样本进行估计时，可能会出现概率为0的情况，使分类出现偏差。&lt;/p&gt;

&lt;p&gt;在计算概率时，可以在分子加一，并在分母加分类数量，来避免这种情况。这种方式叫拉普拉斯平滑。&lt;/p&gt;
</description>
        <pubDate>Tue, 18 Aug 2020 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2020/08/18/Statistical-learning-method-Ch4-Naive-Bayes/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/08/18/Statistical-learning-method-Ch4-Naive-Bayes/</guid>
        
        <category>Statistical learning method</category>
        
        <category>Study notes</category>
        
        <category>Naive Bayes</category>
        
        
      </item>
    
      <item>
        <title>统计学习方法读书笔记 Ch3 k近邻法（k-nearist neighbor）</title>
        <description>&lt;h1 id=&quot;模型&quot;&gt;模型&lt;/h1&gt;

&lt;p&gt;k近邻算法可以用于分类和回归问题，这里对其在分类方面的应用进行讨论。作为一种分类问题的解决方法，k近邻法接收待分类实例特征的向量作为输入，对该实例的分类结果作为输出。具体定义如下：&lt;/p&gt;

&lt;p&gt;存在训练集：&lt;/p&gt;

\[T = \lbrace (x_1, y_1), ..., (x_N, y_N) \rbrace\]

&lt;p&gt;输入实例 $ x $，其中 $ x, x_i \subseteq R^n, y \in \lbrace c_1, …, c_K \rbrace $，$ K $ 为类别的个数，k近邻算法会根据样本集中，与 $ x $ 距离最接近的 $ k $ 个样本，通过某种策略（例如多数表决法）判断出其类别 $ y $。&lt;/p&gt;

&lt;p&gt;如果采用多数表决法，则 $ y $ 的取值可以表示为：&lt;/p&gt;

\[y = \arg\max_{c_j} \sum_{x_i \in N_k(x)} I(y_i = c_j)\]

&lt;p&gt;其中， $ N_k(x) $表示 $ x $ 的邻域，即和 $ x $ 最接近的 $ k $ 个样本构成的集合； $ I(y_i == c_j) $ 为指示函数，当 $ y_i = c_j $ 时为1，否则为0。&lt;/p&gt;

&lt;p&gt;k近邻算法没有显式的训练过程。&lt;/p&gt;

&lt;h1 id=&quot;要素&quot;&gt;要素&lt;/h1&gt;

&lt;h2 id=&quot;距离度量&quot;&gt;距离度量&lt;/h2&gt;

&lt;p&gt;两个向量的距离反映了它们之间的相似度（参考&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E6%80%A7&quot;&gt;余弦相似度&lt;/a&gt;)。k近邻算法一般接受 $ x \in R^n $作为参数，所以可以选用 $ p $ 范数作为距离，一般采用 $ p = 2$，但 $ p $也可以取其他值（ $ p \ge 1 $ ）。&lt;/p&gt;

&lt;p&gt;一般的 $ p $ 范数表示如下：&lt;/p&gt;

\[L_p(x_i, x_j) = \Bigg( \sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p \Bigg)^{\frac{1}{p}}\]

&lt;p&gt;其中，$ x_i， x_j \subseteq R^n $。&lt;/p&gt;

&lt;p&gt;当 $ p = 1 $ 时，称为曼哈顿距离：&lt;/p&gt;

\[L_1(x_i, x_j) = \sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|\]

&lt;p&gt;当 $ p = 2 $ 时，称为欧几里得距离：&lt;/p&gt;

\[L_2(x_i, x_j) = \Bigg( \sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^2 \Bigg)^{\frac{1}{2}}\]

&lt;p&gt;当 $ p = \infty $ 时，范数表示所有坐标中距离的最大值：&lt;/p&gt;

\[L_\infty(x_i, x_j) = \max_l |x_{i}^{l}-x_{j}^{l}|\]

&lt;h2 id=&quot;-k--值的选择&quot;&gt;$ k $ 值的选择&lt;/h2&gt;

&lt;p&gt;$ k $ 值的选取一定程度上反映了模型的复杂度， $ k $ 越大表明模型越简单，$ k $越小表明模型越复杂。&lt;/p&gt;

&lt;p&gt;当 $ k $ 较小时，k近邻算法会从样本集中选取和输入实例距离最接近的较少几个样本通过某种策略对输入实例的类型进行估计，这种方式对样本集中的噪音极为敏感，如果邻域中的样本恰好为噪音，则预测结果不可靠。&lt;/p&gt;

&lt;p&gt;当 $ k $ 较大时，算法会根据较大的邻域中样本的类别对输入实例的类型进行估计，这种情况下样本的噪声会被平均，影响较小，但如果邻域选择较大，其它不相似的样本也会参与决策，使得预测结果不可靠。&lt;/p&gt;

&lt;p&gt;考虑两种极端情况：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;当 $ k = 1 $ 时，输入实例的类型仅有离它最近的样本决定，此时称为最近邻算法。&lt;/li&gt;
  &lt;li&gt;当 $ k = N $ 时，所有的样本均参与决策，如果采用多数表决法，则输出为训练集中存在最多的类别。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;决策规则&quot;&gt;决策规则&lt;/h2&gt;

&lt;p&gt;k近邻算法一般采用多数表决（等权），即将在邻域中出现最多的类别作为实例类别输出。&lt;/p&gt;

&lt;p&gt;k近邻算法还可以采用加权算法，例如对邻域中距离实例较远的点赋予较低的权重，距离较近的点赋予较高的权重。&lt;/p&gt;
</description>
        <pubDate>Fri, 14 Aug 2020 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2020/08/14/Statistical-learning-method-Ch3-KNN/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/08/14/Statistical-learning-method-Ch3-KNN/</guid>
        
        <category>Statistical learning method</category>
        
        <category>Study notes</category>
        
        <category>kNN</category>
        
        <category>k-nearist neighbor</category>
        
        
      </item>
    
      <item>
        <title>统计学习方法读书笔记 Ch2 感知机（Perceptron）</title>
        <description>&lt;h1 id=&quot;模型&quot;&gt;模型&lt;/h1&gt;

&lt;p&gt;感知机是一种二分类的线性模型，其输入为实例特征的向量，输出分类结果。具体定义如下：&lt;/p&gt;

&lt;p&gt;假设输入空间 $ X \subseteq R^n $ ，输出空间 $ Y = \lbrace +1, -1 \rbrace $ ，输入 $ x \in X $ 为输入特征的向量，输出 $ y \in Y $ 为输入向量的分类，则函数&lt;/p&gt;

\[f(x)=sign( \omega \cdot x+b)\]

&lt;p&gt;称为感知机，$ \omega \subseteq R^n $ 和 $ b \subseteq R $ 是感知机模型的参数，它们分别代表权重（权值）和偏置。sign为符号函数，定义为：&lt;/p&gt;

\[sign(x) =
\begin{cases}
+1, &amp;amp; x \ge 0 \\
-1, &amp;amp; x &amp;lt; 0
\end{cases}\]

&lt;h1 id=&quot;几何解释&quot;&gt;几何解释&lt;/h1&gt;

&lt;p&gt;感知机在训练完成后得到方程&lt;/p&gt;

\[\omega\cdot x +b = 0\]

&lt;p&gt;对应在 $ R^n $ 空间中的超平面，其中 $ \omega $ 为超平面的法向量， $ b $ 为截距。该超平面将空间分为两部分，位于不同部分的样本分别为正例和负例。&lt;/p&gt;

&lt;h1 id=&quot;学习策略&quot;&gt;学习策略&lt;/h1&gt;

&lt;h2 id=&quot;线性可分性&quot;&gt;线性可分性&lt;/h2&gt;

&lt;p&gt;对于数据集&lt;/p&gt;

\[T=\lbrace (x_1, y_1), ..., (x_N, y_N) \rbrace\]

&lt;p&gt;其中 $ x \subseteq R^n $ ，$ y \subseteq \lbrace +1, -1 \rbrace $ ，如果存在超平面S&lt;/p&gt;

\[\omega\cdot x + b = 0\]

&lt;p&gt;可以将数据集中的正例和负例完全正确的区分，则称该数据集线性可分。&lt;/p&gt;

&lt;h2 id=&quot;感知机学习策略&quot;&gt;感知机学习策略&lt;/h2&gt;

&lt;p&gt;假设数据集线性可分，那么感知机的目的就是寻找可以将数据集中的正例负例完全正确区分的超平面S。我们可以定义一个损失函数作为学习策略，表示感知机分类错误的严重情况，然后最小化这个损失函数。&lt;/p&gt;

&lt;p&gt;一个自然的想法是将损失函数定义为误分类的数量，但是这个函数并不关于 $ \omega $ 和 $ b $ 可导，难以优化。&lt;/p&gt;

&lt;p&gt;另一个想法是将误分类的样本到超平面距离的总和定义为损失函数，这种方式是感知机所采用的。&lt;/p&gt;

&lt;p&gt;首先定义样本到超平面的距离。已知 $ \omega $ 是超平面的法向量，假设 $ P(x) $ 为超平面上一点，$ Q(x_0) $为样本点，则PQ的距离&lt;/p&gt;

\[\begin{align}
d =&amp;amp; |\frac{\vec{PQ}\cdot\omega}{||\omega||_{2}}| \\
=&amp;amp; \frac{\omega}{||\omega||_{2}}|x_0-x| \\
=&amp;amp;\frac{1}{||\omega||_{2}}|\omega\cdot x_0 - \omega \cdot x| \\
=&amp;amp; \frac{1}{||\omega||_{2}}|\omega\cdot x_0 + b|
\end{align}\]

&lt;p&gt;所以，对于误分类数据 $ (x_i, y_i) $ 而言，可以定义其到超平面的距离：&lt;/p&gt;

\[\frac{1}{||\omega||_{2}}y_{i}(\omega\cdot x_{i} + b) &amp;gt; 0\]

&lt;p&gt;由于 $ \frac{1}{||\omega||_2} $仅对数据进行等比例缩放，所以不予考虑。设 $ M $ 为误分类集合，最终得到损失函数为：&lt;/p&gt;

\[L(\omega, b) = -\sum_{x_i\in M}y_i(\omega\cdot x_i + b)\]

&lt;h1 id=&quot;学习算法原始形式&quot;&gt;学习算法（原始形式）&lt;/h1&gt;

&lt;p&gt;对于数据集&lt;/p&gt;

\[T=\lbrace (x_1, y_1), ..., (x_N, y_N) \rbrace\]

&lt;p&gt;感知机算法就是对最小化问题&lt;/p&gt;

\[\min_{\omega, b} L(\omega, b) = -\sum_{x_i\in M}y_i(\omega\cdot x_i + b)\]

&lt;p&gt;的求解。&lt;/p&gt;

&lt;p&gt;由于函数的梯度方向代表自变量使函数值增长的方向，所以在初始化参数 $ \omega $ 和 $ b $ 后可以采用梯度下降法对上述最小化问题求解。损失函数的梯度为：&lt;/p&gt;

\[\nabla_\omega L(\omega , b) = -\sum_{x_i \in M} y_i x_i \\
\nabla_b L(\omega , b) = -\sum_{x_i \in M} y_i\]

&lt;p&gt;其中，M为误分类点的集合。&lt;/p&gt;

&lt;p&gt;所以如果采用随机梯度下降法，可以采用以下方式更新参数：&lt;/p&gt;

\[\omega := \omega + \lambda y_i x_i \\
b := b + \lambda y_i\]

&lt;p&gt;其中 $ (x_i, y_i) \in M $ ，$ 0 \lt \lambda \le 1 $ ， $ \lambda $为学习率，控制参数更新的尺度。&lt;/p&gt;
</description>
        <pubDate>Tue, 11 Aug 2020 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2020/08/11/Statistical-learning-method-Ch2-Perceptron/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/08/11/Statistical-learning-method-Ch2-Perceptron/</guid>
        
        <category>Statistical learning method</category>
        
        <category>Study notes</category>
        
        <category>Perceptron</category>
        
        
      </item>
    
  </channel>
</rss>
